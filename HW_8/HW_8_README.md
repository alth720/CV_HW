# CV_HW_8

## ğŸ“‚ ê³¼ì œ 1 â€“  SORT ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•œ ë‹¤ì¤‘ ê°ì²´ ì¶”ì ê¸° êµ¬í˜„

### ğŸ“Œ ì£¼ìš” ì½”ë“œ
```python
# í´ë˜ìŠ¤ ì´ë¦„ ë¶ˆëŸ¬ì˜¤ê¸°
with open("model/coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]

# YOLOv4 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
net = cv2.dnn.readNet("model/yolov4.weights", "model/yolov4.cfg")
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# SORT ì¶”ì ê¸° ì´ˆê¸°í™”
tracker = Sort()

# ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°
video_path = "HW_8/slow_traffic_small.mp4"
cap = cv2.VideoCapture(video_path)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    height, width = frame.shape[:2]

    # YOLO ì…ë ¥ ì „ì²˜ë¦¬
    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)
    net.setInput(blob)
    layer_outputs = net.forward(output_layers)

    # íƒì§€ ê²°ê³¼ ì €ì¥
    boxes = []
    confidences = []
    class_ids = []

    for output in layer_outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                center_x, center_y, w, h = (detection[0:4] * np.array([width, height, width, height])).astype("int")
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, int(w), int(h)])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # ë¹„ìµœëŒ€ ì–µì œ(NMS) ì ìš©
    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)
    dets = []
    if len(indices) > 0:
        for i in indices.flatten():
            x, y, w, h = boxes[i]
            dets.append([x, y, x + w, y + h, confidences[i]])
    dets = np.array(dets)

    # SORT ì¶”ì ê¸° ì—…ë°ì´íŠ¸
    tracked_objects = tracker.update(dets)

    # ì¶”ì ëœ ê°ì²´ ì‹œê°í™”
    for *bbox, obj_id in tracked_objects:
        x1, y1, x2, y2 = map(int, bbox)
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, f'ID {int(obj_id)}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    # ê²°ê³¼ í”„ë ˆì„ ì¶œë ¥
    cv2.imshow("YOLOv4 + SORT Tracker", frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
```
```python
ì‹¤í–‰ ë°©ë²•
model/ í´ë”ì— ì•„ë˜ 3ê°œì˜ íŒŒì¼ì„ ë„£ëŠ”ë‹¤.
yolov4.cfg, yolov4.weights, coco.names 

í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜: pip install opencv-python numpy
ì‹¤í–‰: python tracker_yolov4_sort.py
```

### âœ… êµ¬í˜„ ê²°ê³¼
![ìŠ¤í¬ë¦°ìƒ· 2025-04-16 112814](https://github.com/user-attachments/assets/7123a0fa-785f-4679-aa36-5ade16a67e5a)

![ìŠ¤í¬ë¦°ìƒ· 2025-04-16 112848](https://github.com/user-attachments/assets/9226a9df-5cf0-4016-91f8-4d5f7653f53a)



## ğŸ“‚ ê³¼ì œ 2 â€“ Mediapipeë¥¼ í™œìš©í•œ ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì¶œ ë° ì‹œê°í™”

### ğŸ“Œ ì£¼ìš” ì½”ë“œ
```python
import cv2
import mediapipe as mp
#OpenCVë¡œ ì›¹ìº  ì˜ìƒ ì²˜ë¦¬. Googleì˜ MediaPipe ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì–¼êµ´ì˜ 468ê°œ ëœë“œë§ˆí¬ ê²€ì¶œ.

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,          # ì‹¤ì‹œê°„ ì²˜ë¦¬ (False)
    max_num_faces=1,                  # í•œ í™”ë©´ì—ì„œ íƒì§€í•  ì–¼êµ´ ìˆ˜
    refine_landmarks=True,           # ëˆˆ, ì…ìˆ , í™ì±„ ì •ë°€ ì¶”ì¶œ
    min_detection_confidence=0.5     # ìµœì†Œ íƒì§€ ì‹ ë¢°ë„
)
# ì‹¤ì‹œê°„ ì˜ìƒì—ì„œ ì–¼êµ´ì„ ìµœëŒ€ 1ëª…ê¹Œì§€ ì¸ì‹í•˜ë„ë¡ ì„¤ì •. ëˆˆ/ì…/í™ì±„ì˜ ì„¸ë°€í•œ í¬ì¸íŠ¸ê¹Œì§€ ì¶”ì¶œí•˜ëŠ” refine_landmarks ê¸°ëŠ¥ ì‚¬ìš©.

mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
# MediaPipeì—ì„œ ì œê³µí•˜ëŠ” ìŠ¤íƒ€ì¼ ê¸°ë°˜ ì‹œê°í™” ìœ í‹¸ì„ ë¶ˆëŸ¬ì˜´.

cap = cv2.VideoCapture(0)  # ê¸°ë³¸ ì¹´ë©”ë¼ ì—´ê¸°

while cap.isOpened():
    ret, frame = cap.read()
    ...
# ì›¹ìº ìœ¼ë¡œë¶€í„° í”„ë ˆì„ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì•„ì˜´.

rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
results = face_mesh.process(rgb_frame)
# OpenCVëŠ” ê¸°ë³¸ì ìœ¼ë¡œ BGR ì±„ë„ì´ë¯€ë¡œ, FaceMesh ì…ë ¥ì„ ìœ„í•´ RGBë¡œ ë³€í™˜. FaceMesh ëª¨ë¸ì— í˜„ì¬ í”„ë ˆì„ì„ ë„£ì–´ ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì¶œ ìˆ˜í–‰.

if results.multi_face_landmarks:
    for face_landmarks in results.multi_face_landmarks:
        for idx, landmark in enumerate(face_landmarks.landmark):
            x = int(landmark.x * w)
            y = int(landmark.y * h)
            cv2.circle(annotated_frame, (x, y), 1, (0, 255, 0), -1)
# ê²€ì¶œëœ ì–¼êµ´ë§ˆë‹¤ 468ê°œì˜ ëœë“œë§ˆí¬ë¥¼ (x, y) ì¢Œí‘œë¡œ ë³€í™˜í•˜ì—¬ ì´ˆë¡ ì ìœ¼ë¡œ í‘œì‹œ. landmark.xì™€ landmark.yëŠ” 0~1 ì‚¬ì´ì˜ ìƒëŒ€ ì¢Œí‘œì´ë¯€ë¡œ, ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ê³±í•´ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜í•¨.

cv2.imshow("Face Mesh (Press ESC to Exit)", annotated_frame)
if cv2.waitKey(1) == 27:
    break
#ESC í‚¤(27)ë¥¼ ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ ì¢…ë£Œ. ì‹¤ì‹œê°„ìœ¼ë¡œ ì–¼êµ´ ìœ„ì— ëœë“œë§ˆí¬ ì ì´ ê·¸ë ¤ì§„ ì˜ìƒ í‘œì‹œ.
```

### âœ… êµ¬í˜„ ê²°ê³¼
+ <img width="475" alt="image" src="https://github.com/user-attachments/assets/788bff6b-f2b6-4cef-9a97-170c2c68801a" />
